from huggingface_hub import login, upload_folder, snapshot_download, HfApi, hf_hub_download
import os
import shutil
import time
import pandas as pd
import hashlib
import json
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

HF_TOKEN = os.getenv("hf_token")
USERNAME = "ahashanahmed"
REPO_NAME = "csv"
REPO_ID = f"{USERNAME}/{REPO_NAME}"

# ==================== ‡¶¨‡ßá‡¶∏‡¶ø‡¶ï ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ====================

def hf_login(token=None):
    """Hugging Face ‡¶≤‡¶ó‡¶á‡¶®"""
    if token:
        try:
            login(token=token)
            print("üîê HF login ‡¶∏‡¶´‡¶≤ ‡¶π‡ßü‡ßá‡¶õ‡ßá‡•§")
            return True
        except Exception as e:
            print(f"‚ùå HF login ‡¶¨‡ßç‡¶Ø‡¶∞‡ßç‡¶•: {e}")
            return False
    return False

def is_valid_directory(local_dir: str) -> bool:
    """‡¶°‡¶ø‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ø ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡¶ø‡¶° ‡¶ï‡¶ø‡¶®‡¶æ ‡¶ö‡ßá‡¶ï"""
    return os.path.isdir(local_dir) and len(os.listdir(local_dir)) > 0

def create_repo_if_not_exists(repo_id: str = REPO_ID, token: str = HF_TOKEN):
    """‡¶∞‡¶ø‡¶™‡ßã‡¶ú‡¶ø‡¶ü‡¶∞‡¶ø ‡¶§‡ßà‡¶∞‡¶ø (‡¶Ø‡¶¶‡¶ø ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡ßá)"""
    api = HfApi()
    try:
        api.repo_info(repo_id=repo_id, repo_type="dataset", token=token)
        print(f"‚ÑπÔ∏è Repo '{repo_id}' ‡¶Ü‡¶ó‡ßá ‡¶•‡ßá‡¶ï‡ßá‡¶á ‡¶Ü‡¶õ‡ßá‡•§")
        return True
    except Exception:
        try:
            api.create_repo(repo_id=repo_id, repo_type="dataset", private=False, token=token)
            print(f"‚úÖ ‡¶®‡¶§‡ßÅ‡¶® Repo ‡¶§‡ßà‡¶∞‡¶ø ‡¶π‡ßü‡ßá‡¶õ‡ßá: {repo_id}")
            return True
        except Exception as e:
            print(f"‚ùå Repo ‡¶§‡ßà‡¶∞‡¶ø ‡¶¨‡ßç‡¶Ø‡¶∞‡ßç‡¶•: {e}")
            return False

# ==================== ‡¶∏‡ßç‡¶Æ‡¶æ‡¶∞‡ßç‡¶ü ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ ====================

class SmartDatasetUploader:
    """‡¶∏‡ßç‡¶Æ‡¶æ‡¶∞‡ßç‡¶ü ‡¶°‡¶æ‡¶ü‡¶æ‡¶∏‡ßá‡¶ü ‡¶Ü‡¶™‡¶≤‡ßã‡¶°‡¶æ‡¶∞ - ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶ø‡¶§ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßá"""
    
    def __init__(self, repo_id=REPO_ID, token=HF_TOKEN):
        self.api = HfApi()
        self.repo_id = repo_id
        self.token = token
        self.metadata_file = ".dataset_metadata.json"
        self.stats = {
            'total_files': 0,
            'new_files': 0,
            'modified_files': 0,
            'unchanged_files': 0,
            'failed_files': 0
        }
    
    def get_file_hash(self, file_path):
        """‡¶´‡¶æ‡¶á‡¶≤‡ßá‡¶∞ MD5 ‡¶π‡ßç‡¶Ø‡¶æ‡¶∂ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            print(f"‚ö†Ô∏è ‡¶π‡ßç‡¶Ø‡¶æ‡¶∂ ‡¶ó‡¶£‡¶®‡¶æ ‡¶¨‡ßç‡¶Ø‡¶∞‡ßç‡¶•: {e}")
            return None
    
    def get_remote_metadata(self):
        """HF ‡¶•‡ßá‡¶ï‡ßá ‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡¶æ‡¶ü‡¶æ ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶°"""
        try:
            # ‡¶ü‡ßá‡¶Æ‡ßç‡¶™ ‡¶´‡¶æ‡¶á‡¶≤‡ßá ‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡¶æ‡¶ü‡¶æ ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶°
            temp_metadata = f"temp_{self.metadata_file}"
            hf_hub_download(
                repo_id=self.repo_id,
                filename=self.metadata_file,
                repo_type="dataset",
                token=self.token,
                local_path=temp_metadata
            )
            
            with open(temp_metadata, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            os.remove(temp_metadata)
            print(f"üìã ‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡¶æ‡¶ü‡¶æ ‡¶™‡¶æ‡¶ì‡ßü‡¶æ ‡¶ó‡ßá‡¶õ‡ßá: {len(metadata.get('files', {}))} ‡¶ü‡¶ø ‡¶´‡¶æ‡¶á‡¶≤‡ßá‡¶∞ ‡¶§‡¶•‡ßç‡¶Ø")
            return metadata
            
        except Exception as e:
            print(f"üìã ‡¶ï‡ßã‡¶® ‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡¶æ‡¶ü‡¶æ ‡¶®‡ßá‡¶á‡•§ ‡¶®‡¶§‡ßÅ‡¶® ‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡¶æ‡¶ü‡¶æ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ ‡¶π‡¶¨‡ßá‡•§")
            return {
                "files": {}, 
                "last_sync": None,
                "created_at": datetime.now().isoformat()
            }
    
    def upload_metadata(self, metadata):
        """‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡¶æ‡¶ü‡¶æ HF-‡¶è ‡¶Ü‡¶™‡¶≤‡ßã‡¶°"""
        try:
            # ‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡¶æ‡¶ü‡¶æ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶§‡ßà‡¶∞‡¶ø
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            # ‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡¶æ‡¶ü‡¶æ ‡¶Ü‡¶™‡¶≤‡ßã‡¶°
            self.api.upload_file(
                path_or_fileobj=self.metadata_file,
                path_in_repo=self.metadata_file,
                repo_id=self.repo_id,
                repo_type="dataset",
                token=self.token
            )
            
            os.remove(self.metadata_file)
            print(f"üìã ‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡¶æ‡¶ü‡¶æ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶∏‡¶´‡¶≤")
            return True
            
        except Exception as e:
            print(f"‚ö†Ô∏è ‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡¶æ‡¶ü‡¶æ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶¨‡ßç‡¶Ø‡¶∞‡ßç‡¶•: {e}")
            return False
    
    def merge_csv_files(self, local_path, remote_filename, unique_columns=None):
        """
        ‡¶¶‡ßÅ‡¶á CSV ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Æ‡¶æ‡¶∞‡ßç‡¶ú ‡¶ï‡¶∞‡ßá
        - unique_columns: ‡¶Ø‡ßá ‡¶ï‡¶≤‡¶æ‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶§‡ßá ‡¶°‡ßÅ‡¶™‡ßç‡¶≤‡¶ø‡¶ï‡ßá‡¶ü ‡¶∞‡¶ø‡¶Æ‡ßÅ‡¶≠ ‡¶π‡¶¨‡ßá (‡¶Ø‡ßá‡¶Æ‡¶®: ['id', 'timestamp'])
        """
        temp_remote = f"temp_remote_{int(time.time())}.csv"
        
        try:
            # ‡ßß. ‡¶≤‡ßã‡¶ï‡¶æ‡¶≤ CSV ‡¶™‡¶°‡¶º‡¶ø
            local_df = pd.read_csv(local_path)
            print(f"   ‡¶≤‡ßã‡¶ï‡¶æ‡¶≤ ‡¶°‡¶æ‡¶ü‡¶æ: {len(local_df)} ‡¶∞‡ßã")
            
            # ‡ß®. ‡¶∞‡¶ø‡¶Æ‡ßã‡¶ü CSV ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßá ‡¶™‡¶°‡¶º‡¶ø
            try:
                hf_hub_download(
                    repo_id=self.repo_id,
                    filename=remote_filename,
                    repo_type="dataset",
                    token=self.token,
                    local_path=temp_remote
                )
                
                remote_df = pd.read_csv(temp_remote)
                print(f"   ‡¶∞‡¶ø‡¶Æ‡ßã‡¶ü ‡¶°‡¶æ‡¶ü‡¶æ: {len(remote_df)} ‡¶∞‡ßã")
                
                # ‡ß©. ‡¶°‡¶æ‡¶ü‡¶æ ‡¶Æ‡¶æ‡¶∞‡ßç‡¶ú
                if unique_columns and all(col in remote_df.columns for col in unique_columns):
                    # ‡¶á‡¶â‡¶®‡¶ø‡¶ï ‡¶ï‡¶≤‡¶æ‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶§‡ßá ‡¶Æ‡¶æ‡¶∞‡ßç‡¶ú
                    combined_df = pd.concat([remote_df, local_df], ignore_index=True)
                    
                    # ‡¶°‡ßÅ‡¶™‡ßç‡¶≤‡¶ø‡¶ï‡ßá‡¶ü ‡¶∞‡¶ø‡¶Æ‡ßÅ‡¶≠ (‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶®‡¶§‡ßÅ‡¶®‡¶ü‡¶æ ‡¶∞‡¶æ‡¶ñ‡¶¨‡ßá)
                    if 'timestamp' in combined_df.columns:
                        combined_df = combined_df.sort_values('timestamp', ascending=False)
                    
                    combined_df = combined_df.drop_duplicates(
                        subset=unique_columns, 
                        keep='first'
                    )
                    print(f"   ‡¶°‡ßÅ‡¶™‡ßç‡¶≤‡¶ø‡¶ï‡ßá‡¶ü ‡¶∞‡¶ø‡¶Æ‡ßÅ‡¶≠‡ßá‡¶∞ ‡¶™‡¶∞: {len(combined_df)} ‡¶∞‡ßã")
                    
                else:
                    # ‡¶á‡¶â‡¶®‡¶ø‡¶ï ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶¨‡¶æ ‡¶®‡¶æ ‡¶Æ‡¶ø‡¶≤‡¶≤‡ßá ‡¶∏‡¶¨ ‡¶°‡¶æ‡¶ü‡¶æ ‡¶∞‡¶æ‡¶ñ‡¶ø
                    combined_df = pd.concat([remote_df, local_df], ignore_index=True)
                    combined_df = combined_df.drop_duplicates(keep='last')
                    print(f"   ‡¶∏‡¶¨ ‡¶°‡¶æ‡¶ü‡¶æ ‡¶Æ‡¶æ‡¶∞‡ßç‡¶ú: {len(combined_df)} ‡¶∞‡ßã")
                
                # ‡¶ü‡ßá‡¶Æ‡ßç‡¶™ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶°‡¶ø‡¶≤‡¶ø‡¶ü
                if os.path.exists(temp_remote):
                    os.remove(temp_remote)
                
                return combined_df
                
            except Exception as e:
                print(f"   ‡¶∞‡¶ø‡¶Æ‡ßã‡¶ü ‡¶´‡¶æ‡¶á‡¶≤ ‡¶®‡ßá‡¶á, ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶≤‡ßã‡¶ï‡¶æ‡¶≤ ‡¶°‡¶æ‡¶ü‡¶æ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶π‡¶¨‡ßá")
                if os.path.exists(temp_remote):
                    os.remove(temp_remote)
                return local_df
                
        except Exception as e:
            print(f"‚ö†Ô∏è ‡¶Æ‡¶æ‡¶∞‡ßç‡¶ú‡¶ø‡¶Ç ‡¶¨‡ßç‡¶Ø‡¶∞‡ßç‡¶•: {e}")
            if os.path.exists(temp_remote):
                os.remove(temp_remote)
            return None
    
    def upload_file_with_retry(self, file_path, filename, retries=3, delay=2):
        """‡¶∞‡¶ø‡¶ü‡ßç‡¶∞‡¶æ‡¶á ‡¶∏‡¶π ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶≤‡ßã‡¶°"""
        for attempt in range(1, retries + 1):
            try:
                self.api.upload_file(
                    path_or_fileobj=file_path,
                    path_in_repo=filename,
                    repo_id=self.repo_id,
                    repo_type="dataset",
                    token=self.token
                )
                return True
            except Exception as e:
                print(f"   ‚è≥ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ {attempt} ‡¶¨‡ßç‡¶Ø‡¶∞‡ßç‡¶•: {e}")
                if attempt < retries:
                    time.sleep(delay * attempt)
        return False
    
    def smart_upload(self, local_folder="./csv", unique_columns=None):
        """
        ‡¶∏‡ßç‡¶Æ‡¶æ‡¶∞‡ßç‡¶ü ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®
        - unique_columns: CSV ‡¶Æ‡¶æ‡¶∞‡ßç‡¶ú‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶á‡¶â‡¶®‡¶ø‡¶ï ‡¶ï‡¶≤‡¶æ‡¶Æ (‡¶Ø‡ßá‡¶Æ‡¶®: ['id'])
        """
        
        # ‡ß¶. ‡¶™‡ßç‡¶∞‡¶ø-‡¶ö‡ßá‡¶ï
        if not hf_login(self.token):
            return False
        
        if not create_repo_if_not_exists(self.repo_id, self.token):
            return False
        
        if not is_valid_directory(local_folder):
            print(f"‚ö†Ô∏è ‡¶Ü‡¶™‡¶≤‡ßã‡¶°‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶´‡ßã‡¶≤‡ßç‡¶°‡¶æ‡¶∞ ‡¶ñ‡¶æ‡¶≤‡¶ø ‡¶¨‡¶æ ‡¶®‡ßá‡¶á: {local_folder}")
            return False
        
        print(f"\n{'='*60}")
        print(f"üöÄ ‡¶∏‡ßç‡¶Æ‡¶æ‡¶∞‡ßç‡¶ü ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶∂‡ßÅ‡¶∞‡ßÅ: {local_folder}")
        print(f"{'='*60}\n")
        
        # ‡ßß. ‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡¶æ‡¶ü‡¶æ ‡¶≤‡ßã‡¶°
        metadata = self.get_remote_metadata()
        remote_files = metadata.get('files', {})
        
        # ‡ß®. ‡¶≤‡ßã‡¶ï‡¶æ‡¶≤ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶∏‡ßç‡¶ï‡ßç‡¶Ø‡¶æ‡¶®
        local_files = {}
        csv_files = [f for f in os.listdir(local_folder) if f.endswith('.csv')]
        
        print(f"\nüìÅ ‡¶≤‡ßã‡¶ï‡¶æ‡¶≤ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶∏‡ßç‡¶ï‡ßç‡¶Ø‡¶æ‡¶®‡¶ø‡¶Ç...")
        for filename in csv_files:
            file_path = os.path.join(local_folder, filename)
            
            if os.path.isfile(file_path):
                file_hash = self.get_file_hash(file_path)
                file_size = os.path.getsize(file_path)
                modified_time = os.path.getmtime(file_path)
                
                if file_hash:
                    local_files[filename] = {
                        'hash': file_hash,
                        'size': file_size,
                        'modified': modified_time,
                        'modified_str': datetime.fromtimestamp(modified_time).strftime('%Y-%m-%d %H:%M:%S'),
                        'path': file_path
                    }
        
        self.stats['total_files'] = len(local_files)
        print(f"\nüìä ‡¶´‡¶æ‡¶á‡¶≤ ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£:")
        print(f"   ‡¶Æ‡ßã‡¶ü CSV ‡¶´‡¶æ‡¶á‡¶≤: {self.stats['total_files']}")
        
        # ‡ß©. ‡¶´‡¶æ‡¶á‡¶≤ ‡¶§‡ßÅ‡¶≤‡¶®‡¶æ
        files_to_process = []
        
        for filename, local_info in local_files.items():
            remote_info = remote_files.get(filename, {})
            
            if filename not in remote_files:
                # ‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶®‡¶§‡ßÅ‡¶® ‡¶´‡¶æ‡¶á‡¶≤
                files_to_process.append(('new', filename, local_info))
                self.stats['new_files'] += 1
                print(f"   üÜï ‡¶®‡¶§‡ßÅ‡¶® ‡¶´‡¶æ‡¶á‡¶≤: {filename} ({local_info['size']/1024:.1f}KB)")
                
            elif local_info['hash'] != remote_info.get('hash'):
                # ‡¶´‡¶æ‡¶á‡¶≤ ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶ø‡¶§ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá
                files_to_process.append(('modified', filename, local_info))
                self.stats['modified_files'] += 1
                print(f"   üìù ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶ø‡¶§: {filename} ({local_info['size']/1024:.1f}KB)")
                
            else:
                # ‡¶Ö‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶ø‡¶§ ‡¶´‡¶æ‡¶á‡¶≤
                self.stats['unchanged_files'] += 1
        
        print(f"\nüîÑ ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶ï‡¶∞‡¶£ ‡¶∂‡ßÅ‡¶∞‡ßÅ...\n")
        
        # ‡ß™. ‡¶´‡¶æ‡¶á‡¶≤ ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶ï‡¶∞‡¶£
        for change_type, filename, local_info in files_to_process:
            try:
                print(f"üìÑ {filename}:")
                
                if change_type == 'modified':
                    # ‡¶Æ‡¶æ‡¶∞‡ßç‡¶ú‡¶ø‡¶Ç ‡¶∏‡¶π ‡¶Ü‡¶™‡¶≤‡ßã‡¶°
                    print(f"   ‡¶Æ‡¶æ‡¶∞‡ßç‡¶ú ‡¶ï‡¶∞‡¶õ‡¶ø...")
                    merged_df = self.merge_csv_files(
                        local_info['path'], 
                        filename,
                        unique_columns
                    )
                    
                    if merged_df is not None:
                        # ‡¶Æ‡¶æ‡¶∞‡ßç‡¶ú ‡¶ï‡¶∞‡¶æ ‡¶°‡¶æ‡¶ü‡¶æ ‡¶ü‡ßá‡¶Æ‡ßç‡¶™ ‡¶´‡¶æ‡¶á‡¶≤‡ßá ‡¶∏‡ßá‡¶≠
                        temp_file = f"temp_merged_{int(time.time())}_{filename}"
                        merged_df.to_csv(temp_file, index=False, encoding='utf-8')
                        
                        # ‡¶Ü‡¶™‡¶≤‡ßã‡¶°
                        print(f"   ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶õ‡¶ø...")
                        if self.upload_file_with_retry(temp_file, filename):
                            # ‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡¶æ‡¶ü‡¶æ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü
                            metadata['files'][filename] = {
                                'hash': local_info['hash'],
                                'size': local_info['size'],
                                'modified': local_info['modified'],
                                'last_upload': datetime.now().isoformat(),
                                'merged': True
                            }
                            print(f"   ‚úÖ ‡¶Æ‡¶æ‡¶∞‡ßç‡¶ú ‡¶ì ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶∏‡¶´‡¶≤")
                        else:
                            self.stats['failed_files'] += 1
                            print(f"   ‚ùå ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶¨‡ßç‡¶Ø‡¶∞‡ßç‡¶•")
                        
                        # ‡¶ü‡ßá‡¶Æ‡ßç‡¶™ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶°‡¶ø‡¶≤‡¶ø‡¶ü
                        if os.path.exists(temp_file):
                            os.remove(temp_file)
                    else:
                        self.stats['failed_files'] += 1
                        print(f"   ‚ùå ‡¶Æ‡¶æ‡¶∞‡ßç‡¶ú ‡¶¨‡ßç‡¶Ø‡¶∞‡ßç‡¶•")
                
                else:  # ‡¶®‡¶§‡ßÅ‡¶® ‡¶´‡¶æ‡¶á‡¶≤
                    print(f"   ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶õ‡¶ø...")
                    if self.upload_file_with_retry(local_info['path'], filename):
                        # ‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡¶æ‡¶ü‡¶æ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü
                        metadata['files'][filename] = {
                            'hash': local_info['hash'],
                            'size': local_info['size'],
                            'modified': local_info['modified'],
                            'last_upload': datetime.now().isoformat(),
                            'merged': False
                        }
                        print(f"   ‚úÖ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶∏‡¶´‡¶≤")
                    else:
                        self.stats['failed_files'] += 1
                        print(f"   ‚ùå ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶¨‡ßç‡¶Ø‡¶∞‡ßç‡¶•")
                
                print()
                
            except Exception as e:
                self.stats['failed_files'] += 1
                print(f"   ‚ùå ‡¶è‡¶∞‡¶∞: {str(e)}\n")
        
        # ‡ß´. ‡¶Æ‡ßá‡¶ü‡¶æ‡¶°‡¶æ‡¶ü‡¶æ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü
        metadata['last_sync'] = datetime.now().isoformat()
        self.upload_metadata(metadata)
        
        # ‡ß¨. ‡¶∏‡¶æ‡¶∞‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡ßá‡¶™
        print(f"\n{'='*60}")
        print(f"üìä ‡¶ö‡ßÇ‡¶°‡¶º‡¶æ‡¶®‡ßç‡¶§ ‡¶∏‡¶æ‡¶∞‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡ßá‡¶™:")
        print(f"{'='*60}")
        print(f"   ‡¶Æ‡ßã‡¶ü ‡¶´‡¶æ‡¶á‡¶≤: {self.stats['total_files']}")
        print(f"   ‡¶®‡¶§‡ßÅ‡¶® ‡¶´‡¶æ‡¶á‡¶≤: {self.stats['new_files']}")
        print(f"   ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶ø‡¶§ ‡¶´‡¶æ‡¶á‡¶≤: {self.stats['modified_files']}")
        print(f"   ‡¶Ö‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶ø‡¶§: {self.stats['unchanged_files']}")
        print(f"   ‡¶¨‡ßç‡¶Ø‡¶∞‡ßç‡¶•: {self.stats['failed_files']}")
        print(f"{'='*60}\n")
        
        return self.stats['failed_files'] == 0

# ==================== ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ====================

def simple_upload(folder_path="./csv", repo_id=REPO_ID, token=HF_TOKEN, retries=3, delay=5):
    """‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶´‡ßã‡¶≤‡ßç‡¶°‡¶æ‡¶∞ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° (‡¶∏‡¶¨ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßá)"""
    hf_login(token)
    create_repo_if_not_exists(repo_id, token)

    if not is_valid_directory(folder_path):
        print(f"‚ö†Ô∏è ‡¶Ü‡¶™‡¶≤‡ßã‡¶°‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶´‡ßã‡¶≤‡ßç‡¶°‡¶æ‡¶∞ ‡¶ñ‡¶æ‡¶≤‡¶ø ‡¶¨‡¶æ ‡¶®‡ßá‡¶á: {folder_path}")
        return False

    print(f"üì§ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶∂‡ßÅ‡¶∞‡ßÅ: {folder_path} ‚Üí {repo_id}")
    for attempt in range(1, retries + 1):
        try:
            upload_folder(folder_path=folder_path, repo_id=repo_id, repo_type="dataset", token=token)
            print("‚úÖ HF ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶∏‡¶´‡¶≤ ‡¶π‡ßü‡ßá‡¶õ‡ßá‡•§")
            return True
        except Exception as e:
            print(f"‚è≥ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ {attempt} ‡¶¨‡ßç‡¶Ø‡¶∞‡ßç‡¶•: {e}")
            time.sleep(delay)

    print("‚ùå HF ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£‡¶≠‡¶æ‡¶¨‡ßá ‡¶¨‡ßç‡¶Ø‡¶∞‡ßç‡¶• ‡¶π‡ßü‡ßá‡¶õ‡ßá‡•§")
    return False

# ==================== ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ====================

def download_from_hf(local_dir="./csv", repo_id=REPO_ID, token=HF_TOKEN):
    """HF ‡¶•‡ßá‡¶ï‡ßá ‡¶°‡¶æ‡¶ü‡¶æ‡¶∏‡ßá‡¶ü ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶°"""
    create_repo_if_not_exists(repo_id, token)

    if is_valid_directory(local_dir):
        print("‚úÖ ‡¶≤‡ßã‡¶ï‡¶æ‡¶≤ './csv' ‡¶´‡ßã‡¶≤‡ßç‡¶°‡¶æ‡¶∞ ‡¶Ü‡¶ó‡ßá ‡¶•‡ßá‡¶ï‡ßá‡¶á ‡¶Ü‡¶õ‡ßá‡•§")
        return True

    print("üîç ‡¶≤‡ßã‡¶ï‡¶æ‡¶≤ './csv' ‡¶´‡ßã‡¶≤‡ßç‡¶°‡¶æ‡¶∞ ‡¶®‡ßá‡¶á, HF ‡¶•‡ßá‡¶ï‡ßá ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡¶õ‡¶ø...")

    try:
        hf_login(token)
        temp_path = snapshot_download(repo_id=repo_id, repo_type="dataset", token=token)

        def copy_contents(src_dir, dst_dir):
            os.makedirs(dst_dir, exist_ok=True)
            for item in os.listdir(src_dir):
                s = os.path.join(src_dir, item)
                d = os.path.join(dst_dir, item)
                if os.path.isdir(s):
                    shutil.copytree(s, d, dirs_exist_ok=True)
                elif os.path.isfile(s):
                    shutil.copy2(s, d)

        copy_contents(temp_path, local_dir)
        print("‚úÖ HF ‡¶•‡ßá‡¶ï‡ßá ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶∏‡¶´‡¶≤, './csv' ‡¶´‡ßã‡¶≤‡ßç‡¶°‡¶æ‡¶∞‡ßá ‡¶ï‡¶™‡¶ø ‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶®‡•§")
        return True

    except Exception as e:
        print(f"‚ö†Ô∏è HF ‡¶•‡ßá‡¶ï‡ßá ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶¨‡ßç‡¶Ø‡¶∞‡ßç‡¶•: {e}")
        return False

# ==================== ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞‡ßá‡¶∞ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ ====================

if __name__ == "__main__":
    
    # ‡¶Ö‡¶™‡¶∂‡¶® ‡ßß: ‡¶∏‡ßç‡¶Æ‡¶æ‡¶∞‡ßç‡¶ü ‡¶Ü‡¶™‡¶≤‡ßã‡¶° (‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶ø‡¶§ ‡¶´‡¶æ‡¶á‡¶≤)
    print("\nüîß ‡¶∏‡ßç‡¶Æ‡¶æ‡¶∞‡ßç‡¶ü ‡¶Ü‡¶™‡¶≤‡ßã‡¶°‡¶æ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞:")
    uploader = SmartDatasetUploader(REPO_ID, HF_TOKEN)
    
    # CSV ‡¶Æ‡¶æ‡¶∞‡ßç‡¶ú‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶á‡¶â‡¶®‡¶ø‡¶ï ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ (‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ CSV ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®)
    unique_columns = ['id']  # ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ CSV-‡¶è‡¶∞ ‡¶™‡ßç‡¶∞‡¶æ‡¶á‡¶Æ‡¶æ‡¶∞‡¶ø ‡¶ï‡¶ø
    
    uploader.smart_upload(
        local_folder="./csv",
        unique_columns=unique_columns  # ‡¶á‡¶â‡¶®‡¶ø‡¶ï ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶∏‡ßá‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®
    )
    
    # ‡¶Ö‡¶™‡¶∂‡¶® ‡ß®: ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° (‡¶∏‡¶¨ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶≤‡ßã‡¶°)
    # simple_upload("./csv")
    
    # ‡¶Ö‡¶™‡¶∂‡¶® ‡ß©: ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶°
    # download_from_hf("./csv")